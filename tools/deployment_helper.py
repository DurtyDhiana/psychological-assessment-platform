#!/usr/bin/env python3\n\"\"\"\nDeployment Helper\nGenerates deployment configurations and infrastructure code from blueprint\n\"\"\"\n\nimport json\nimport yaml\nfrom typing import Dict, Any\nfrom pathlib import Path\n\nclass DeploymentHelper:\n    def __init__(self, blueprint_path: str):\n        with open(blueprint_path, 'r') as f:\n            self.blueprint = json.load(f)\n        self.model_config = self.blueprint.get(\"modelBlueprint\", {})\n    \n    def generate_docker_compose(self) -> str:\n        \"\"\"Generate Docker Compose configuration\"\"\"\n        tech_config = self.model_config.get(\"technicalConfiguration\", {})\n        architecture = tech_config.get(\"architecture\", {})\n        \n        compose_config = {\n            \"version\": \"3.8\",\n            \"services\": {\n                \"frontend\": {\n                    \"build\": \"./frontend\",\n                    \"ports\": [\"3000:3000\"],\n                    \"environment\": [\n                        \"REACT_APP_API_URL=http://backend:5000\"\n                    ],\n                    \"depends_on\": [\"backend\"]\n                },\n                \"backend\": {\n                    \"build\": \"./backend\",\n                    \"ports\": [\"5000:5000\"],\n                    \"environment\": [\n                        \"FLASK_ENV=production\",\n                        \"DATABASE_URL=firebase://your-project\",\n                        f\"GPT_TEMPERATURE={tech_config.get('modelParameters', {}).get('temperature', 0.7)}\",\n                        f\"GPT_MAX_TOKENS={tech_config.get('modelParameters', {}).get('maxTokens', 2000)}\"\n                    ],\n                    \"depends_on\": [\"redis\"]\n                },\n                \"redis\": {\n                    \"image\": \"redis:alpine\",\n                    \"ports\": [\"6379:6379\"]\n                }\n            }\n        }\n        \n        return yaml.dump(compose_config, default_flow_style=False)\n    \n    def generate_aws_lambda_config(self) -> Dict[str, Any]:\n        \"\"\"Generate AWS Lambda deployment configuration\"\"\"\n        tech_config = self.model_config.get(\"technicalConfiguration\", {})\n        \n        lambda_config = {\n            \"AWSTemplateFormatVersion\": \"2010-09-09\",\n            \"Transform\": \"AWS::Serverless-2016-10-31\",\n            \"Resources\": {\n                \"AssessmentAPI\": {\n                    \"Type\": \"AWS::Serverless::Function\",\n                    \"Properties\": {\n                        \"CodeUri\": \"backend/\",\n                        \"Handler\": \"app.lambda_handler\",\n                        \"Runtime\": \"python3.9\",\n                        \"Timeout\": 30,\n                        \"MemorySize\": 512,\n                        \"Environment\": {\n                            \"Variables\": {\n                                \"GPT_TEMPERATURE\": str(tech_config.get(\"modelParameters\", {}).get(\"temperature\", 0.7)),\n                                \"GPT_MAX_TOKENS\": str(tech_config.get(\"modelParameters\", {}).get(\"maxTokens\", 2000))\n                            }\n                        },\n                        \"Events\": {\n                            \"AssessmentAPI\": {\n                                \"Type\": \"Api\",\n                                \"Properties\": {\n                                    \"Path\": \"/api/{proxy+}\",\n                                    \"Method\": \"ANY\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        \n        return lambda_config\n    \n    def generate_kubernetes_manifests(self) -> Dict[str, str]:\n        \"\"\"Generate Kubernetes deployment manifests\"\"\"\n        manifests = {}\n        \n        # Backend deployment\n        backend_deployment = {\n            \"apiVersion\": \"apps/v1\",\n            \"kind\": \"Deployment\",\n            \"metadata\": {\"name\": \"assessment-backend\"},\n            \"spec\": {\n                \"replicas\": 3,\n                \"selector\": {\"matchLabels\": {\"app\": \"assessment-backend\"}},\n                \"template\": {\n                    \"metadata\": {\"labels\": {\"app\": \"assessment-backend\"}},\n                    \"spec\": {\n                        \"containers\": [{\n                            \"name\": \"backend\",\n                            \"image\": \"your-registry/assessment-backend:latest\",\n                            \"ports\": [{\"containerPort\": 5000}],\n                            \"env\": [\n                                {\"name\": \"FLASK_ENV\", \"value\": \"production\"},\n                                {\"name\": \"GPT_TEMPERATURE\", \"value\": \"0.7\"}\n                            ]\n                        }]\n                    }\n                }\n            }\n        }\n        \n        # Backend service\n        backend_service = {\n            \"apiVersion\": \"v1\",\n            \"kind\": \"Service\",\n            \"metadata\": {\"name\": \"assessment-backend-service\"},\n            \"spec\": {\n                \"selector\": {\"app\": \"assessment-backend\"},\n                \"ports\": [{\"port\": 80, \"targetPort\": 5000}],\n                \"type\": \"ClusterIP\"\n            }\n        }\n        \n        manifests[\"backend-deployment.yaml\"] = yaml.dump(backend_deployment)\n        manifests[\"backend-service.yaml\"] = yaml.dump(backend_service)\n        \n        return manifests\n    \n    def generate_environment_config(self, environment: str = \"production\") -> Dict[str, Any]:\n        \"\"\"Generate environment-specific configuration\"\"\"\n        tech_config = self.model_config.get(\"technicalConfiguration\", {})\n        model_params = tech_config.get(\"modelParameters\", {})\n        \n        config = {\n            \"environment\": environment,\n            \"model\": {\n                \"temperature\": model_params.get(\"temperature\", 0.7),\n                \"top_p\": model_params.get(\"topP\", 0.9),\n                \"max_tokens\": model_params.get(\"maxTokens\", 2000)\n            },\n            \"api\": {\n                \"base_url\": f\"https://api-{environment}.yourapp.com\",\n                \"version\": \"v1\",\n                \"rate_limit\": \"1000/hour\" if environment == \"production\" else \"100/hour\"\n            },\n            \"database\": {\n                \"type\": \"firebase\",\n                \"project_id\": f\"your-project-{environment}\"\n            },\n            \"security\": {\n                \"encryption\": \"AES-256\",\n                \"jwt_expiry\": \"24h\",\n                \"cors_origins\": [\"https://yourapp.com\"] if environment == \"production\" else [\"*\"]\n            }\n        }\n        \n        return config\n    \n    def generate_monitoring_config(self) -> Dict[str, Any]:\n        \"\"\"Generate monitoring and alerting configuration\"\"\"\n        performance_metrics = self.model_config.get(\"qualityAssurance\", {}).get(\"performanceMetrics\", {})\n        \n        monitoring_config = {\n            \"prometheus\": {\n                \"scrape_configs\": [{\n                    \"job_name\": \"assessment-api\",\n                    \"static_configs\": [{\"targets\": [\"backend:5000\"]}],\n                    \"metrics_path\": \"/metrics\",\n                    \"scrape_interval\": \"30s\"\n                }]\n            },\n            \"alerts\": {\n                \"response_time\": {\n                    \"threshold\": \"3s\",\n                    \"description\": \"API response time exceeds threshold\"\n                },\n                \"accuracy\": {\n                    \"threshold\": \"85%\",\n                    \"description\": \"Assessment accuracy below threshold\"\n                },\n                \"error_rate\": {\n                    \"threshold\": \"5%\",\n                    \"description\": \"Error rate exceeds acceptable limit\"\n                }\n            },\n            \"dashboards\": {\n                \"grafana\": {\n                    \"panels\": [\n                        \"API Response Times\",\n                        \"Assessment Completion Rates\",\n                        \"User Satisfaction Scores\",\n                        \"System Resource Usage\"\n                    ]\n                }\n            }\n        }\n        \n        return monitoring_config\n    \n    def save_deployment_files(self, output_dir: str):\n        \"\"\"Save all deployment files to specified directory\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        # Docker Compose\n        with open(output_path / \"docker-compose.yml\", \"w\") as f:\n            f.write(self.generate_docker_compose())\n        \n        # AWS Lambda\n        with open(output_path / \"lambda-template.yaml\", \"w\") as f:\n            yaml.dump(self.generate_aws_lambda_config(), f)\n        \n        # Kubernetes\n        k8s_manifests = self.generate_kubernetes_manifests()\n        k8s_dir = output_path / \"kubernetes\"\n        k8s_dir.mkdir(exist_ok=True)\n        for filename, content in k8s_manifests.items():\n            with open(k8s_dir / filename, \"w\") as f:\n                f.write(content)\n        \n        # Environment configs\n        for env in [\"development\", \"staging\", \"production\"]:\n            config = self.generate_environment_config(env)\n            with open(output_path / f\"config-{env}.json\", \"w\") as f:\n                json.dump(config, f, indent=2)\n        \n        # Monitoring\n        with open(output_path / \"monitoring.yaml\", \"w\") as f:\n            yaml.dump(self.generate_monitoring_config(), f)\n        \n        print(f\"Deployment files generated in: {output_path}\")\n\n# CLI usage\nif __name__ == \"__main__\":\n    import sys\n    \n    if len(sys.argv) != 3:\n        print(\"Usage: python deployment_helper.py <blueprint_file> <output_directory>\")\n        sys.exit(1)\n    \n    blueprint_file = sys.argv[1]\n    output_dir = sys.argv[2]\n    \n    helper = DeploymentHelper(blueprint_file)\n    helper.save_deployment_files(output_dir)\n