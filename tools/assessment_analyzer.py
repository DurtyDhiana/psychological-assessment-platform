#!/usr/bin/env python3\n\"\"\"\nAssessment Analyzer\nAnalyzes and visualizes the comprehensive assessment suite\n\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom datetime import datetime\n\nclass AssessmentAnalyzer:\n    def __init__(self, blueprint_path):\n        with open(blueprint_path, 'r') as f:\n            self.blueprint = json.load(f)\n        self.assessments = self.blueprint[\"modelBlueprint\"][\"assessments\"][\"available\"]\n        self.simulations = self.blueprint[\"modelBlueprint\"].get(\"simulations\", [])\n    \n    def generate_comprehensive_report(self):\n        \"\"\"Generate a detailed analysis report\"\"\"\n        \n        report = []\n        report.append(\"# ðŸ§  Comprehensive Assessment Suite Analysis Report\")\n        report.append(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        report.append(f\"**Total Assessments:** {len(self.assessments)}\")\n        report.append(f\"**Total Simulations:** {len(self.simulations)}\")\n        report.append(\"\")\n        \n        # Category Analysis\n        report.append(\"## ðŸ“Š Assessment Distribution by Category\")\n        category_stats = self._analyze_categories()\n        \n        for category, data in category_stats.items():\n            if data['count'] > 0:\n                report.append(f\"### {category.title()} ({data['count']} assessments)\")\n                report.append(f\"- **Average Questions:** {data['avg_questions']:.1f}\")\n                report.append(f\"- **Average Duration:** {data['avg_duration']}\")\n                report.append(f\"- **Simulation-Enabled:** {data['simulation_count']}/{data['count']}\")\n                report.append(\"\")\n                \n                for assessment in data['assessments']:\n                    report.append(f\"**{assessment['title']}**\")\n                    report.append(f\"- *{assessment['description']}*\")\n                    report.append(f\"- Questions: {assessment['metadata']['questionCount']}\")\n                    report.append(f\"- Duration: {assessment['metadata']['estimatedDuration']}\")\n                    report.append(f\"- Framework: {assessment['metadata'].get('psychologicalFramework', 'N/A')}\")\n                    report.append(f\"- Traits: {', '.join(assessment['metadata']['traitsMeasured'])}\")\n                    report.append(\"\")\n        \n        # Simulation Analysis\n        if self.simulations:\n            report.append(\"## ðŸŽ® Interactive Simulations\")\n            for sim in self.simulations:\n                report.append(f\"### {sim['title']}\")\n                report.append(f\"- *{sim['description']}*\")\n                report.append(f\"- **Difficulty:** {sim['configuration']['difficultyLevel'].title()}\")\n                report.append(f\"- **Type:** {sim['configuration']['interactionType'].replace('_', ' ').title()}\")\n                report.append(f\"- **Duration:** {sim['configuration']['estimatedDuration']}\")\n                report.append(f\"- **Related Assessments:** {', '.join(sim['relatedAssessments'])}\")\n                report.append(\"\")\n        \n        # Psychological Frameworks\n        report.append(\"## ðŸ”¬ Psychological Frameworks Used\")\n        frameworks = self._analyze_frameworks()\n        for framework, count in frameworks.items():\n            report.append(f\"- **{framework}:** {count} assessment(s)\")\n        report.append(\"\")\n        \n        # Time Investment Analysis\n        report.append(\"## â±ï¸ Time Investment Analysis\")\n        time_stats = self._analyze_time_investment()\n        report.append(f\"- **Total Assessment Time:** ~{time_stats['total_minutes']} minutes ({time_stats['total_hours']:.1f} hours)\")\n        report.append(f\"- **Average per Assessment:** {time_stats['avg_minutes']:.1f} minutes\")\n        report.append(f\"- **Shortest Assessment:** {time_stats['shortest']} minutes\")\n        report.append(f\"- **Longest Assessment:** {time_stats['longest']} minutes\")\n        report.append(\"\")\n        \n        # Trait Coverage Analysis\n        report.append(\"## ðŸŽ¯ Psychological Trait Coverage\")\n        trait_coverage = self._analyze_trait_coverage()\n        report.append(f\"**Total Unique Traits Measured:** {len(trait_coverage)}\")\n        report.append(\"\")\n        \n        # Group traits by frequency\n        trait_frequency = sorted(trait_coverage.items(), key=lambda x: x[1], reverse=True)\n        \n        report.append(\"### Most Frequently Assessed Traits:\")\n        for trait, count in trait_frequency[:10]:\n            report.append(f\"- **{trait.replace('_', ' ').title()}:** {count} assessment(s)\")\n        report.append(\"\")\n        \n        # Recommendations\n        report.append(\"## ðŸ’¡ Recommendations & Next Steps\")\n        recommendations = self._generate_recommendations()\n        for rec in recommendations:\n            report.append(f\"- {rec}\")\n        report.append(\"\")\n        \n        # Technical Specifications\n        report.append(\"## ðŸ”§ Technical Specifications\")\n        tech_specs = self._analyze_technical_specs()\n        for spec, value in tech_specs.items():\n            report.append(f\"- **{spec}:** {value}\")\n        \n        return \"\\n\".join(report)\n    \n    def _analyze_categories(self):\n        \"\"\"Analyze assessments by category\"\"\"\n        categories = defaultdict(lambda: {\n            'count': 0, \n            'assessments': [], \n            'total_questions': 0,\n            'simulation_count': 0,\n            'durations': []\n        })\n        \n        for assessment in self.assessments:\n            cat = assessment['category']\n            categories[cat]['count'] += 1\n            categories[cat]['assessments'].append(assessment)\n            categories[cat]['total_questions'] += assessment['metadata']['questionCount']\n            \n            if assessment['features']['simulationSupported']:\n                categories[cat]['simulation_count'] += 1\n            \n            # Extract duration (assuming format like \"10-12 minutes\")\n            duration_str = assessment['metadata']['estimatedDuration']\n            categories[cat]['durations'].append(duration_str)\n        \n        # Calculate averages\n        for cat, data in categories.items():\n            if data['count'] > 0:\n                data['avg_questions'] = data['total_questions'] / data['count']\n                data['avg_duration'] = data['durations'][0] if data['durations'] else \"N/A\"\n        \n        return dict(categories)\n    \n    def _analyze_frameworks(self):\n        \"\"\"Analyze psychological frameworks used\"\"\"\n        frameworks = defaultdict(int)\n        \n        for assessment in self.assessments:\n            framework = assessment['metadata'].get('psychologicalFramework')\n            if framework and framework != 'N/A':\n                frameworks[framework] += 1\n        \n        return dict(frameworks)\n    \n    def _analyze_time_investment(self):\n        \"\"\"Analyze time investment across all assessments\"\"\"\n        total_minutes = 0\n        durations = []\n        \n        for assessment in self.assessments:\n            duration_str = assessment['metadata']['estimatedDuration']\n            # Extract average minutes (assuming format like \"10-12 minutes\")\n            try:\n                if '-' in duration_str:\n                    min_time, max_time = duration_str.split('-')\n                    min_val = int(min_time.strip())\n                    max_val = int(max_time.split()[0].strip())\n                    avg_duration = (min_val + max_val) / 2\n                else:\n                    avg_duration = int(duration_str.split()[0])\n                \n                durations.append(avg_duration)\n                total_minutes += avg_duration\n            except:\n                durations.append(12)  # Default fallback\n                total_minutes += 12\n        \n        return {\n            'total_minutes': int(total_minutes),\n            'total_hours': total_minutes / 60,\n            'avg_minutes': total_minutes / len(self.assessments) if self.assessments else 0,\n            'shortest': min(durations) if durations else 0,\n            'longest': max(durations) if durations else 0\n        }\n    \n    def _analyze_trait_coverage(self):\n        \"\"\"Analyze psychological trait coverage\"\"\"\n        trait_count = defaultdict(int)\n        \n        for assessment in self.assessments:\n            for trait in assessment['metadata']['traitsMeasured']:\n                trait_count[trait] += 1\n        \n        return dict(trait_count)\n    \n    def _analyze_technical_specs(self):\n        \"\"\"Analyze technical specifications\"\"\"\n        total_questions = sum(a['metadata']['questionCount'] for a in self.assessments)\n        simulation_enabled = sum(1 for a in self.assessments if a['features']['simulationSupported'])\n        \n        return {\n            'Total Questions Across All Assessments': total_questions,\n            'Simulation-Enabled Assessments': f\"{simulation_enabled}/{len(self.assessments)}\",\n            'Average Questions per Assessment': f\"{total_questions/len(self.assessments):.1f}\",\n            'Adaptive Logic Enabled': f\"{sum(1 for a in self.assessments if a['features']['adaptiveLogic'])}/{len(self.assessments)}\",\n            'Exportable Results': f\"{sum(1 for a in self.assessments if a['features']['exportable'])}/{len(self.assessments)}\",\n            'Progress Tracking': f\"{sum(1 for a in self.assessments if a['features']['progressTracking'])}/{len(self.assessments)}\"\n        }\n    \n    def _generate_recommendations(self):\n        \"\"\"Generate recommendations based on analysis\"\"\"\n        recommendations = []\n        \n        # Category balance\n        category_stats = self._analyze_categories()\n        empty_categories = [cat for cat, data in category_stats.items() if data['count'] == 0]\n        \n        if empty_categories:\n            recommendations.append(f\"Consider adding assessments to empty categories: {', '.join(empty_categories)}\")\n        \n        # Simulation coverage\n        simulation_enabled = sum(1 for a in self.assessments if a['features']['simulationSupported'])\n        if simulation_enabled < len(self.assessments) * 0.7:\n            recommendations.append(\"Consider adding more simulation scenarios to increase interactive learning\")\n        \n        # Time balance\n        time_stats = self._analyze_time_investment()\n        if time_stats['longest'] - time_stats['shortest'] > 15:\n            recommendations.append(\"Consider balancing assessment durations for better user experience\")\n        \n        # Framework diversity\n        frameworks = self._analyze_frameworks()\n        if len(frameworks) < len(self.assessments) * 0.5:\n            recommendations.append(\"Consider diversifying psychological frameworks for broader theoretical coverage\")\n        \n        # Always include these\n        recommendations.extend([\n            \"Implement A/B testing for question effectiveness\",\n            \"Add multilingual support for global deployment\",\n            \"Create assessment recommendation engine based on user goals\",\n            \"Develop mobile-optimized versions for better accessibility\",\n            \"Implement real-time analytics dashboard for administrators\"\n        ])\n        \n        return recommendations\n\ndef create_visual_dashboard():\n    \"\"\"Create visual dashboard of assessment data\"\"\"\n    print(\"ðŸ“Š Creating Visual Dashboard...\")\n    \n    analyzer = AssessmentAnalyzer(\"../blueprints/comprehensive_assessment_suite.json\")\n    \n    # Generate and save the report\n    report = analyzer.generate_comprehensive_report()\n    \n    with open(\"../documentation/comprehensive_assessment_analysis.md\", \"w\") as f:\n        f.write(report)\n    \n    print(\"âœ… Analysis report saved to: ../documentation/comprehensive_assessment_analysis.md\")\n    \n    return analyzer\n\nif __name__ == \"__main__\":\n    analyzer = create_visual_dashboard()\n    print(\"\\nðŸŽ‰ Assessment analysis complete!\")\n    print(\"ðŸ“‹ Check the documentation folder for the detailed report.\")\n